import requests
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
from email.header import Header
import os
import datetime

# --- é…ç½® ---
# (ä¸ºäº†æµ‹è¯•ï¼Œæˆ‘ä»¬æš‚æ—¶ä¸å‘é‚®ä»¶ï¼Œåªæµ‹è¯•æŠ“å–å’Œå†™å…¥)
TARGETS = [
    {
        "name": "åŠ›å·¥å­¦é™¢-é€šçŸ¥å…¬å‘Š",
        "url": "https://smes.shu.edu.cn/tzgg.htm",
        "selector": "div.main_conR ul li a, div.list_right ul li a, .list ul li a" # å¢åŠ äº†å¤šä¸ªå¤‡é€‰è§„åˆ™
    },
    {
        "name": "ä¸Šå¤§ç ”ç©¶ç”Ÿé™¢-å…¬å‘Š",
        "url": "https://gs.shu.edu.cn/tzgg.htm",
        "selector": "div.list ul li a, .list_r ul li a" 
    }
]
HISTORY_FILE = "history.txt"

def run_debug():
    print(f"[{datetime.datetime.now()}] === å¼€å§‹è°ƒè¯•æ¨¡å¼ ===")
    
    # 1. æ£€æŸ¥æ–‡ä»¶æƒé™
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    if os.path.exists(HISTORY_FILE):
        print("history.txt æ–‡ä»¶å­˜åœ¨ã€‚")
    else:
        print("history.txt æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‡†å¤‡åˆ›å»ºã€‚")

    all_links = []
    
    # æ¨¡æ‹Ÿæ›´çœŸå®çš„æµè§ˆå™¨å¤´ï¼Œé˜²æ­¢è¢«æ‹¦æˆª
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
    }

    for target in TARGETS:
        print(f"\næ­£åœ¨å°è¯•è¿æ¥: {target['name']} -> {target['url']}")
        try:
            resp = requests.get(target['url'], headers=headers, timeout=20)
            resp.encoding = 'utf-8'
            print(f"  > çŠ¶æ€ç : {resp.status_code} (200è¡¨ç¤ºæˆåŠŸ)")
            
            if resp.status_code != 200:
                print("  > âŒ ç½‘é¡µæ— æ³•è®¿é—®ï¼Œè·³è¿‡ã€‚")
                continue

            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # è°ƒè¯•ï¼šæ‰“å°ä¸€ä¸‹ç½‘é¡µé•¿åº¦ï¼Œçœ‹æ˜¯ä¸æ˜¯ç©ºç™½é¡µ
            print(f"  > ç½‘é¡µå†…å®¹é•¿åº¦: {len(resp.text)} å­—ç¬¦")
            
            # å°è¯•æŸ¥æ‰¾é“¾æ¥
            # æ³¨æ„ï¼šè¿™é‡Œå¢åŠ äº†å¤šä¸ªé€‰æ‹©å™¨ï¼Œå°è¯•åŒ¹é…æ›´å¤šæƒ…å†µ
            links = soup.select(target['selector'])
            print(f"  > ğŸ” æ‰¾åˆ°é“¾æ¥æ•°é‡: {len(links)}")
            
            if len(links) == 0:
                print("  > âš ï¸ è­¦å‘Šï¼šæ²¡æ‰¾åˆ°ä»»ä½•é“¾æ¥ï¼å¯èƒ½æ˜¯é€‰æ‹©å™¨é”™äº†ï¼Œæˆ–è€…ç½‘é¡µæœ‰åçˆ¬è™«éªŒè¯ã€‚")
            
            for link in links:
                href = link.get('href')
                title = link.get_text(strip=True)
                if href and len(title) > 2:
                    # ç®€å•æ‹¼æ¥
                    full = href if href.startswith("http") else target['url'].rsplit('/', 1)[0] + '/' + href
                    all_links.append(f"{title} | {full}")
                    # åªæ‰“å°å‰3ä¸ªçœ‹çœ‹å¯¹ä¸å¯¹
                    if len(all_links) <= 3:
                        print(f"    - æŠ“å–æ ·ä¾‹: {title}")

        except Exception as e:
            print(f"  > âŒ å‘ç”Ÿé”™è¯¯: {e}")

    # 2. å¼ºè¡Œå†™å…¥æ–‡ä»¶æµ‹è¯•
    if len(all_links) > 0:
        print(f"\næ­£åœ¨å†™å…¥ {len(all_links)} æ¡æ•°æ®åˆ° history.txt ...")
        try:
            with open(HISTORY_FILE, "w", encoding="utf-8") as f:
                for item in all_links:
                    f.write(item + "\n")
            print("âœ… å†™å…¥å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
    else:
        print("\nâŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œæ–‡ä»¶æœªæ›´æ–°ã€‚")

if __name__ == "__main__":
    run_debug()
