import requests
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
from email.header import Header
import os
import datetime
import urllib3
import time  # å¼•å…¥æ—¶é—´åº“ï¼Œç”¨äºæ§åˆ¶å‘é€é€Ÿåº¦

# å¿½ç•¥è¯ä¹¦é”™è¯¯è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é…ç½®åŒºåŸŸ ---
SMTP_SERVER = "smtp.qq.com"
SMTP_PORT = 465
MAIL_USER = os.getenv("MAIL_USER")
MAIL_PASS = os.getenv("MAIL_PASS")
RECEIVER_EMAIL = os.getenv("MAIL_USER")

HISTORY_FILE = "history.txt"

SCHOOLS = [
    {
        "name": "åŠ›å·¥å­¦é™¢-é€šçŸ¥å…¬å‘Š",
        "urls": ["https://smes.shu.edu.cn/index/tzgg.htm"],
        "selectors": ["div[class*='list'] li a", ".winstyle67696 a", "ul li a"]
    },
    {
        "name": "ä¸Šå¤§ç ”ç©¶ç”Ÿé™¢-ç»¼åˆé€šçŸ¥",
        "urls": [
            "https://gs.shu.edu.cn/index.htm",
            "https://gs.shu.edu.cn/xwzx.htm"
        ],
        # åªè¦é“¾æ¥åŒ…å«è¿™äº›IDï¼Œå°±è§†ä¸ºç›®æ ‡é€šçŸ¥
        "keywords": ["info/1027", "info/1029"],
        "selectors": ["a"] 
    }
]

def send_email(title, link, source_name):
    if not MAIL_USER or not MAIL_PASS:
        return
    try:
        subject = f"ã€æ–°é€šçŸ¥ã€‘{source_name}: {title}"
        content = f"æ¥æº: {source_name}\næ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\næ ‡é¢˜: {title}\né“¾æ¥: {link}"
        
        message = MIMEText(content, 'plain', 'utf-8')
        message['From'] = MAIL_USER
        message['To'] = RECEIVER_EMAIL
        message['Subject'] = Header(subject, 'utf-8')

        server = smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT)
        server.login(MAIL_USER, MAIL_PASS)
        server.sendmail(MAIL_USER, [RECEIVER_EMAIL], message.as_string())
        server.quit()
        print(f"ğŸ“§ [å·²å‘é€] {title}")
        
        # === æ ¸å¿ƒä¿æŠ¤æœºåˆ¶ ===
        # æ¯å‘å®Œä¸€å°ï¼Œå¼ºåˆ¶ä¼‘æ¯ 10 ç§’
        # è¿™æ˜¯ä¸ºäº†é˜²æ­¢ QQ é‚®ç®±æŠŠä½ å½“æˆå‘åƒåœ¾å¹¿å‘Šçš„ç›´æ¥å°å·
        print("   (ä¼‘æ¯ 10 ç§’)...")
        time.sleep(10) 
        
    except Exception as e:
        print(f"âŒ å‘é€å¤±è´¥: {e}")

def run_task():
    print(f"[{datetime.datetime.now()}] å¼€å§‹æŠ“å–...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    # è¯»å–å†å²è®°å½•
    history = set()
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            for line in f:
                history.add(line.strip())
    
    new_history = history.copy()
    has_new = False
    sent_count = 0

    for school in SCHOOLS:
        print(f"\næ­£åœ¨è¿æ¥: {school['name']}")
        soup = None
        used_url = ""

        # å°è¯•è¿æ¥
        for url in school['urls']:
            try:
                print(f"  Trying: {url} ...", end="")
                resp = requests.get(url, headers=headers, timeout=15, verify=False)
                resp.encoding = 'utf-8'
                if resp.status_code == 200 and len(resp.text) > 500:
                    print(" âœ… é€šäº†")
                    soup = BeautifulSoup(resp.text, 'html.parser')
                    used_url = url
                    break
                else:
                    print(f" âŒ {resp.status_code}")
            except:
                print(" âŒ è¶…æ—¶")
        
        if not soup: continue

        # æå–é“¾æ¥
        found_links = []
        if "keywords" in school: # å…³é”®è¯æ¨¡å¼(ç ”ç©¶ç”Ÿé™¢)
            all_a = soup.find_all('a')
            for a in all_a:
                href = a.get('href')
                if href and any(k in href for k in school['keywords']):
                    found_links.append(a)
        else: # é€‰æ‹©å™¨æ¨¡å¼(åŠ›å·¥å­¦é™¢)
            for sel in school['selectors']:
                found_links = soup.select(sel)
                if found_links: break

        print(f"    > æ‰¾åˆ° {len(found_links)} ä¸ªç›¸å…³é“¾æ¥")

        # å€’åºå¤„ç†ï¼ˆè®©æ—§é€šçŸ¥å…ˆå‘ï¼Œæ–°é€šçŸ¥åå‘ï¼Œæˆ–è€…ä¿æŒç½‘é¡µé¡ºåºï¼‰
        # è¿™é‡Œä¿æŒç½‘é¡µé»˜è®¤é¡ºåº
        for link in found_links:
            href = link.get('href')
            title = link.get_text(strip=True)
            
            if not href or len(title) < 4: continue
            
            # é“¾æ¥è¡¥å…¨
            if not href.startswith("http"):
                if href.startswith("/"):
                    domain = "/".join(used_url.split("/")[:3])
                    full_url = domain + href
                else:
                    if href.startswith("info/"):
                         domain = "/".join(used_url.split("/")[:3])
                         full_url = f"{domain}/{href}"
                    else:
                         full_url = used_url.rsplit("/", 1)[0] + "/" + href
            else:
                full_url = href

            # === æ ¸å¿ƒé€»è¾‘ä¿®æ”¹ ===
            # åªè¦ä¸åœ¨ history é‡Œï¼Œå°±å‘é‚®ä»¶ï¼
            # ä¸å†åˆ¤æ–­ "len(history) > 0"
            if full_url not in history:
                send_email(title, full_url, school['name'])
                
                new_history.add(full_url)
                has_new = True
                sent_count += 1

    # å…¨éƒ¨å‘å®Œåï¼Œä¿å­˜è®°å½•
    # è¿™æ ·ä¸‹æ¬¡è¿è¡Œï¼Œè¿™äº›å·²ç»åœ¨ new_history é‡Œçš„å°±ä¸ä¼šå†å‘äº†
    if has_new:
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            for url in sorted(list(new_history)):
                f.write(url + "\n")
        print(f"\nâœ… åˆå§‹åŒ–å®Œæˆï¼å·²å‘é€ {sent_count} å°é‚®ä»¶ï¼Œè®°å½•å·²æ›´æ–°ã€‚")
    else:
        print("\næš‚æ— æ–°å†…å®¹")

if __name__ == "__main__":
    run_task()
